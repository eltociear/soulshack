# connect to a local ollama server via openai compatible API
nick: ollamabot
model: llama3.2:3b
openaiurl: "http://localhost:11434/v1/"
